{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "230a625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import pickle\n",
    "import torch\n",
    "from torch import linalg\n",
    "import numpy as np\n",
    "from scipy.linalg import cho_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "142959af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(repo: str, file: str):\n",
    "    # Path constant to save the object\n",
    "    PATH = f'{repo}/{file}.pkl'\n",
    "\n",
    "    with open(PATH, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "69348109",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_object('../../data/train', 'donut')\n",
    "features = torch.from_numpy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8ff2f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.8302,  1.7729],\n",
      "        [-3.0147,  3.6299],\n",
      "        [-2.7704, -4.4909],\n",
      "        ...,\n",
      "        [-3.2588,  3.8029],\n",
      "        [-4.8496,  2.3319],\n",
      "        [ 3.7889,  3.3190]], dtype=torch.float64)\n",
      "torch.Size([1000, 2])\n",
      "tensor([[ 4.8302, -3.0147, -2.7704,  ..., -3.2588, -4.8496,  3.7889],\n",
      "        [ 1.7729,  3.6299, -4.4909,  ...,  3.8029,  2.3319,  3.3190]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[ 4.8302, -3.0147, -2.7704,  ..., -3.2588, -4.8496,  3.7889],\n",
      "        [ 1.7729,  3.6299, -4.4909,  ...,  3.8029,  2.3319,  3.3190]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(features.shape)\n",
    "reshaped_feats = features.t() #torch.reshape(features, (2, 1000))\n",
    "print(reshaped_feats)\n",
    "print(features.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "61702ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2016,  0.0000],\n",
      "        [-0.5309,  1.6547]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "means = torch.zeros(2, dtype=torch.float64).normal_()\n",
    "sigma = torch.zeros(2, 2, dtype=torch.float64).normal_()\n",
    "L = torch.linalg.cholesky(sigma @ sigma.t() + torch.eye(2))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d4b54369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.4935, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "mahalanobis = lambda x, mu, S_inv: (x - mu).t() @ S_inv @ (x - mu)\n",
    "\n",
    "print(mahalanobis(features[1], means, torch.inverse(L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d870e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis(u, v, cov):\n",
    "    delta = u - v\n",
    "    print(delta.shape)\n",
    "    m = torch.matmul(torch.matmul(delta.t(), torch.inverse(cov)), delta)\n",
    "    return m\n",
    "\n",
    "def _batch_mahalanobis(bL, bx):\n",
    "    r\"\"\"\n",
    "    Computes the squared Mahalanobis distance :math:`\\mathbf{x}^\\top\\mathbf{M}^{-1}\\mathbf{x}`\n",
    "    for a factored :math:`\\mathbf{M} = \\mathbf{L}\\mathbf{L}^\\top`.\n",
    "\n",
    "    Accepts batches for both bL and bx. They are not necessarily assumed to have the same batch\n",
    "    shape, but `bL` one should be able to broadcasted to `bx` one.\n",
    "    \"\"\"\n",
    "    n = bx.size(-1)\n",
    "    bx_batch_shape = bx.shape[:-1]\n",
    "\n",
    "    # Assume that bL.shape = (i, 1, n, n), bx.shape = (..., i, j, n),\n",
    "    # we are going to make bx have shape (..., 1, j,  i, 1, n) to apply batched tri.solve\n",
    "    bx_batch_dims = len(bx_batch_shape)\n",
    "    bL_batch_dims = bL.dim() - 2\n",
    "    outer_batch_dims = bx_batch_dims - bL_batch_dims\n",
    "    old_batch_dims = outer_batch_dims + bL_batch_dims\n",
    "    new_batch_dims = outer_batch_dims + 2 * bL_batch_dims\n",
    "    # Reshape bx with the shape (..., 1, i, j, 1, n)\n",
    "    bx_new_shape = bx.shape[:outer_batch_dims]\n",
    "    for (sL, sx) in zip(bL.shape[:-2], bx.shape[outer_batch_dims:-1]):\n",
    "        bx_new_shape += (sx // sL, sL)\n",
    "    bx_new_shape += (n,)\n",
    "    bx = bx.reshape(bx_new_shape)\n",
    "    # Permute bx to make it have shape (..., 1, j, i, 1, n)\n",
    "    permute_dims = (list(range(outer_batch_dims)) +\n",
    "                    list(range(outer_batch_dims, new_batch_dims, 2)) +\n",
    "                    list(range(outer_batch_dims + 1, new_batch_dims, 2)) +\n",
    "                    [new_batch_dims])\n",
    "    bx = bx.permute(permute_dims)\n",
    "\n",
    "    flat_L = bL.reshape(-1, n, n)  # shape = b x n x n\n",
    "    flat_x = bx.reshape(-1, flat_L.size(0), n)  # shape = c x b x n\n",
    "    flat_x_swap = flat_x.permute(1, 2, 0)  # shape = b x n x c\n",
    "    M_swap = torch.linalg.solve_triangular(flat_L, flat_x_swap, upper=False).pow(2).sum(-2)  # shape = b x c\n",
    "    M = M_swap.t()  # shape = c x b\n",
    "\n",
    "    # Now we revert the above reshape and permute operators.\n",
    "    permuted_M = M.reshape(bx.shape[:-1])  # shape = (..., 1, j, i, 1)\n",
    "    permute_inv_dims = list(range(outer_batch_dims))\n",
    "    for i in range(bL_batch_dims):\n",
    "        permute_inv_dims += [outer_batch_dims + i, old_batch_dims + i]\n",
    "    reshaped_M = permuted_M.permute(permute_inv_dims)  # shape = (..., 1, i, j, 1)\n",
    "    return reshaped_M.reshape(bx_batch_shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c0d7a358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([20.1001,  5.0694, 24.7087, 17.6917,  6.5707, 17.0211, 16.1480, 18.0451,\n",
       "        16.2420, 16.0514,  3.2556,  8.1285, 16.9439, 23.2921, 23.8508, 11.2674,\n",
       "        24.9308,  9.8712, 18.9821, 17.2443, 16.3630, 19.7476, 21.6267, 17.6330,\n",
       "        18.6070, 18.2136, 29.3943,  5.5698, 18.2921, 21.3648, 19.6743,  9.3026,\n",
       "        21.4542, 22.8172, 15.0363, 15.8300,  9.7038, 14.7042, 19.4855, 20.2349,\n",
       "        17.1715,  4.6161, 13.6980, 17.5796,  4.7823, 16.5145, 20.8763,  5.3385,\n",
       "        14.8522,  3.6058, 16.8907, 20.2990, 19.0398,  8.8930, 13.2721, 15.0168,\n",
       "        10.0832,  6.6415, 15.0075,  5.3959, 16.4204, 13.5941,  8.1815, 19.0371,\n",
       "         6.6960, 26.9291, 25.6815, 18.7053, 20.1590, 14.8073, 18.3407, 23.8836,\n",
       "        19.0003,  3.0680, 12.7399, 20.8446, 13.8286, 26.2801, 14.8152, 13.3350,\n",
       "        22.5234, 11.3606,  5.0073, 23.4201, 22.1629, 23.1404, 18.0632, 18.2459,\n",
       "        17.8623, 24.3920, 11.6619, 24.5748, 16.9772, 23.8919, 22.3738, 18.2644,\n",
       "         4.3031, 16.5787, 18.0221,  6.8991, 19.9899,  3.0659,  6.7556, 16.3855,\n",
       "         9.7117,  5.3333, 18.3859, 20.2210, 19.5661, 17.4770,  4.0486, 26.4655,\n",
       "         3.5677,  7.2907, 12.4604, 19.3378,  4.4600, 16.9503, 18.7335, 19.5093,\n",
       "        16.1501, 15.0983,  2.3686, 17.3092,  3.5413, 20.9954, 17.6866, 25.1174,\n",
       "         3.2245, 18.1182, 15.2641, 18.9386, 14.4925,  4.5246, 18.2248, 25.1516,\n",
       "         9.6044,  4.0932, 15.0458, 23.6633, 16.4090, 11.4016, 13.4443, 18.3297,\n",
       "        11.1611, 16.7565, 15.2580,  3.8805, 12.5198, 18.8154,  5.4914, 14.8317,\n",
       "        18.4890, 23.6237, 15.3324, 20.5453, 18.8024,  5.0865, 14.6559, 14.0259,\n",
       "        14.7279,  4.7508, 16.5858, 14.9528, 24.3360, 23.5478, 11.5565, 13.5354,\n",
       "        18.0316, 14.9802, 17.4134, 15.2973,  9.0883, 15.7725, 19.0521, 25.7712,\n",
       "        17.9297,  3.3664, 25.1640, 20.3123, 16.2286, 22.6651, 15.5055, 24.3123,\n",
       "        25.6706, 16.6738,  5.5092, 16.9854, 19.5637, 18.0844,  7.0012, 17.9760,\n",
       "        20.4829,  9.7344, 18.5254,  3.0441,  4.1572,  7.5910, 15.1557, 19.6690,\n",
       "        22.2242, 25.9470, 23.0099, 23.9252,  4.8957, 15.9793, 17.8316,  3.6928,\n",
       "        16.4706,  8.1094, 19.8746, 17.8129, 20.9345, 21.8328, 10.5157, 15.4381,\n",
       "        20.0705, 21.1464,  4.6917, 18.6588, 18.1707, 28.5739,  6.0351, 14.3948,\n",
       "        16.0599, 17.3758,  4.9351,  3.7920, 17.3430,  5.4714,  4.6835, 15.4792,\n",
       "        26.0746,  2.9736, 18.2300, 21.6582,  9.7475,  8.2824, 22.7214, 17.8693,\n",
       "        12.9894,  8.3750, 17.2038, 13.7918, 24.6036, 24.0639, 18.4644, 11.3895,\n",
       "        20.0235,  8.9315, 14.4655, 24.4813, 24.3392, 16.3705, 25.2920, 23.5242,\n",
       "        15.3796, 15.9371, 19.2488, 17.3982, 26.1466, 15.0861, 16.9069, 18.0494,\n",
       "        17.7762, 16.2898, 16.3308, 13.2713, 22.1872, 14.0263,  5.9813, 17.3400,\n",
       "         5.9388,  8.4216, 18.4289, 16.3902, 17.9028,  2.9110, 20.1334, 24.0827,\n",
       "        18.2578, 18.8882,  2.5626,  5.7521, 13.6717, 12.4873, 16.5963, 17.6260,\n",
       "         3.5769, 15.3367, 18.2149,  2.9397, 18.6815,  6.0408,  6.3552, 17.2352,\n",
       "        28.7240, 21.6909, 26.3938, 12.2926, 24.5065, 16.7787,  3.7689, 17.4557,\n",
       "        20.1156, 18.4454, 14.2115,  6.5405, 16.5345, 17.2101, 15.1740,  6.1588,\n",
       "        12.1567, 21.0929,  3.6004, 15.7606, 19.5143, 25.0456, 14.8871, 20.6488,\n",
       "        20.4333, 23.5397, 16.2767,  3.5232, 10.2447, 14.4207, 20.7214, 24.1778,\n",
       "        14.4438, 18.3927,  8.6761, 16.3433, 15.2828,  4.7037,  5.0335,  3.7419,\n",
       "         3.8949, 17.2762,  3.0508, 18.3517, 16.6259,  6.4649,  3.4383,  5.4207,\n",
       "        16.8833,  3.4855,  2.3784, 16.3348, 22.2613, 16.6585, 25.5227, 19.4340,\n",
       "        25.9294, 14.4733, 19.0961,  8.6472, 22.2917, 16.5711,  6.4681, 10.0329,\n",
       "        17.2941, 20.4464, 22.8771,  3.1833, 22.8517, 14.1460, 17.5769, 15.2246,\n",
       "        25.1690,  3.9208, 20.3005, 24.1665, 15.6948, 16.8537, 16.2368, 14.1978,\n",
       "        17.7285, 14.4655, 14.7425, 17.6402, 16.1675, 19.1172,  9.4080, 21.3139,\n",
       "        15.1739, 15.6663,  2.6339, 21.0533, 14.0739, 14.6896, 18.9711, 16.6155,\n",
       "         5.8301, 25.8128, 14.7374, 19.9334, 13.9346, 26.7093, 16.8233,  3.0353,\n",
       "        20.1459,  2.4598, 17.9828, 20.3064,  7.3167, 10.3908, 17.3533,  5.9257,\n",
       "        15.4307,  2.8648, 22.7595, 10.9773, 10.4664,  9.7602, 20.5978, 13.7593,\n",
       "        14.1852,  3.1656, 17.5039, 18.4855, 10.6612,  5.8812,  2.6511, 24.0876,\n",
       "         4.8892, 27.2998,  3.3225, 15.7295, 16.3023,  5.2006, 19.9084, 16.8152,\n",
       "        15.3408, 23.2962,  2.8569, 15.8862, 23.9687, 15.0008, 14.6225, 24.9078,\n",
       "        19.7699, 22.6326, 23.9787, 19.6104, 13.2998,  8.1403,  2.9968,  3.5815,\n",
       "        23.6706, 15.3566, 25.9794, 26.1624, 28.0180,  2.6700, 18.2643, 12.0572,\n",
       "         6.7932, 19.5635, 16.9382, 14.9331, 17.7052, 20.1280, 17.0610, 19.1248,\n",
       "        19.5169, 15.6114, 23.4163, 18.2473, 17.4160,  8.6151, 16.6862,  4.8739,\n",
       "        19.2202, 14.4400, 18.6893, 10.4766, 15.2759,  2.9318, 16.0658, 11.7415,\n",
       "         7.7714, 17.5686, 23.9111, 20.4425, 13.8111, 11.5800, 17.1054, 16.8233,\n",
       "        10.8822, 17.3876, 22.4374, 17.4071, 14.9288, 16.6663,  3.0266,  6.9965,\n",
       "        16.0171, 21.0849, 17.3992, 24.7325, 24.4035, 20.0534,  8.1827, 26.9105,\n",
       "        19.6064,  7.3994,  3.7989, 17.1381,  8.6768,  9.0365, 19.0274, 19.7380,\n",
       "        24.0445, 16.8440, 17.7611, 13.5491, 11.1935, 18.7382,  3.5412,  9.4908,\n",
       "        13.5878, 18.4982,  7.8032, 21.7411,  7.0005, 25.6749,  3.7585, 17.4230,\n",
       "        21.6245, 15.7470, 23.3835, 14.7192, 15.1096, 18.9708, 20.7583,  6.3947,\n",
       "        13.4406, 10.0741,  7.7131, 19.6112, 15.2380,  9.8715, 14.4955, 14.3716,\n",
       "         5.6816,  3.3089, 15.2019,  6.7286,  3.8709, 15.4579, 18.3624, 14.4596,\n",
       "        17.3157, 22.6589, 18.7812, 19.7853,  6.8111, 21.9897,  4.0785,  4.0429,\n",
       "        15.3589, 22.6596,  5.8773, 22.8358,  9.4395, 18.6893,  3.0711, 21.2465,\n",
       "        12.6584,  2.5584, 24.8661,  3.3663, 18.7261, 14.6808, 10.7789, 19.4794,\n",
       "        13.7659, 14.8075, 16.7905,  4.1306,  8.6721, 17.6162, 25.8485, 18.8979,\n",
       "        16.4582, 26.4943, 10.3703, 24.7249,  4.4791, 18.5990,  6.8267, 18.7242,\n",
       "        20.3799, 16.4066, 22.5808, 21.3457, 21.9208, 15.5754, 17.4444, 20.6809,\n",
       "        17.8853, 15.8559, 12.7531,  5.5677, 18.0259, 17.8801, 25.2104,  3.1011,\n",
       "        13.9606, 13.6080,  2.7966, 24.7237, 20.3035, 14.3209,  3.7371, 17.8444,\n",
       "        12.7620, 15.0383, 19.2282, 23.2041, 18.1109,  3.1969, 16.1842,  9.4481,\n",
       "        19.5389, 17.1512, 16.1743,  4.2965, 21.2204,  3.9111, 11.0325, 14.8149,\n",
       "         8.4130, 19.7158, 16.3307, 26.7480, 25.0582, 16.8591, 15.5834, 12.3032,\n",
       "         5.9596,  2.5034, 16.8538,  3.3976, 16.0843, 27.8503, 16.2067,  3.3435,\n",
       "        15.9609, 17.6238,  2.8980,  9.1701, 20.2534, 18.6414, 14.3711, 17.4850,\n",
       "         4.1226, 14.5577, 15.3899, 15.7372, 20.2018, 15.9375,  2.9741, 16.4202,\n",
       "        23.5700,  7.6033,  2.6321, 19.8384, 16.2883, 19.7584, 17.8114, 18.9552,\n",
       "        16.1639, 26.6265,  3.4778,  4.4028, 17.7312, 18.8137, 26.2574, 18.1923,\n",
       "        24.2621, 24.6209,  4.5781, 14.9850,  6.9985,  4.2508,  8.5104, 14.1470,\n",
       "        15.5355, 19.3707, 16.0003, 16.9164,  7.4329, 22.9733,  9.3196, 14.8147,\n",
       "        17.9242, 22.6575, 16.0105,  6.0096, 17.1911,  3.9212, 17.9619, 22.4896,\n",
       "         8.4016,  4.6794, 23.9421, 21.8336,  6.8486, 14.6354, 23.0647, 15.6226,\n",
       "        22.6433,  9.9183, 19.2308, 16.0737, 14.7646,  8.6206, 16.8853,  5.1757,\n",
       "        21.2110,  4.6355, 21.6742, 18.0973,  3.5762, 13.7451, 23.2641,  6.9579,\n",
       "        16.0350, 20.4226, 15.6165, 17.2989, 18.8140, 13.5457, 16.4089, 17.3014,\n",
       "        19.5942, 20.0243, 17.4533, 16.7081, 23.0343, 11.2291, 26.2597, 15.8847,\n",
       "        11.8169, 16.9745,  4.2702,  4.5611,  7.7125,  6.9428, 13.5959, 17.0803,\n",
       "        17.0699, 15.8566, 19.1823, 14.7816, 14.2460, 21.9290, 20.5489, 20.6523,\n",
       "        22.0428, 14.0423, 20.3898, 23.9627, 24.1307,  3.1988,  9.6152, 18.1400,\n",
       "        13.9817, 20.3506, 15.4474,  3.4532, 19.8093, 23.4545, 18.2976, 20.7908,\n",
       "         8.4560,  3.8671, 18.5052, 19.1322, 17.0237, 18.3032,  3.0748, 24.0411,\n",
       "        19.9220, 11.0859, 17.3449, 22.9841, 16.4169, 22.6872, 19.6420, 17.0401,\n",
       "         7.1640, 23.6977, 12.4895, 16.2413,  2.9914, 20.0089, 13.5522, 22.0680,\n",
       "         3.2358, 17.3493, 18.0020, 19.2662, 19.5428, 23.5959, 24.6401,  2.5355,\n",
       "        17.9862,  5.6156, 23.3131, 12.8877,  4.9079, 25.4493, 15.2338, 18.8465,\n",
       "        16.4240,  8.1613, 20.4574, 13.0784, 11.0189, 12.2539, 21.3095, 24.3524,\n",
       "        14.9405, 13.7222,  6.3069,  8.8139,  3.6734, 15.5130, 19.3749, 19.6158,\n",
       "        13.5355, 16.2093, 20.1161, 13.9105, 20.6732, 15.2597,  3.4634, 15.7891,\n",
       "        24.9092,  3.3690, 20.8624, 15.4456, 22.4455, 17.4404, 13.9954,  2.8525,\n",
       "        10.3870, 13.0784, 21.0225,  2.6447, 22.6040, 18.1098, 19.1604, 18.1287,\n",
       "        11.8752,  6.0289, 18.3468,  4.6558, 23.8742, 18.1299, 13.9791, 19.0168,\n",
       "        22.3236, 23.8872, 25.0850, 13.2349, 19.4989, 14.7314, 15.6060, 20.4352,\n",
       "         6.9000, 16.3508, 25.3105, 22.0653,  7.8321, 18.6794, 22.8357, 19.8146,\n",
       "        19.0765, 17.9041, 17.0740, 22.1574, 13.9101, 18.5679, 17.2044, 16.0863,\n",
       "        17.6749, 14.3265,  2.6012, 25.1816,  3.2367, 16.4637,  2.5999, 14.9627,\n",
       "        16.3115, 23.6758,  6.8767, 17.1613, 17.3435, 13.5079, 26.3710, 14.8381,\n",
       "        15.9063,  3.9046,  4.8801, 16.0323, 16.2599,  4.6669, 13.1041, 11.3246,\n",
       "        24.2281, 15.8311, 14.9255, 18.5628,  4.4656, 18.3956, 16.6938, 24.2172,\n",
       "         2.4510, 16.6655, 24.4684, 17.0366, 18.7639, 13.2305, 17.4939, 14.2488,\n",
       "        16.8301, 15.1921, 14.0224, 15.9452, 21.8409, 25.5526,  5.6271,  7.3566,\n",
       "        23.9581, 16.5615,  5.3943, 18.4957,  4.8340, 24.4106, 21.1815, 19.0590,\n",
       "        17.5565, 19.0088, 19.7799, 16.8397,  3.9261, 18.8847, 22.3241,  4.0063,\n",
       "        24.0815, 15.1382, 14.2493, 16.1699, 16.1410, 18.4634, 15.5797, 23.4912,\n",
       "        22.0216, 14.4339, 20.4351, 16.4728,  3.2218,  6.2694, 16.3442, 14.9715,\n",
       "        17.0843, 18.2302, 22.7540, 13.3458, 20.3541,  9.0534, 10.3639,  4.0640,\n",
       "        15.8159, 25.7178, 13.2601, 19.0882, 18.0027, 24.8702, 19.7041,  9.8238,\n",
       "         3.9182, 24.7755, 18.6814, 17.6692, 15.8219, 13.9960,  9.7876, 13.6356,\n",
       "        17.1979, 16.8102, 22.2582, 19.6007, 17.0316,  6.0427, 15.0634, 15.5088],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mahalanobis(features.t(), means.unsqueeze(1), L).shape\n",
    "print(means.shape)\n",
    "_batch_mahalanobis(L, (features - means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "8081f9c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[287], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mreshaped_feats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m cho_solve((L, \u001b[38;5;28;01mTrue\u001b[39;00m), x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "x = reshaped_feats - means\n",
    "y = cho_solve((L, True), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "a4552b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "169e97c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[43mreshaped_feats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m)\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve_triangular(L, (reshaped_feats \u001b[38;5;241m-\u001b[39m means)\u001b[38;5;241m.\u001b[39mt(), upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve_triangular(L\u001b[38;5;241m.\u001b[39mt(), y, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1000) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "print((reshaped_feats - means).t().shape)\n",
    "\n",
    "y = torch.linalg.solve_triangular(L, (reshaped_feats - means).t(), upper=False)\n",
    "y = torch.linalg.solve_triangular(L.t(), y, upper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "51e752be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_determinant_part = -torch.sum(torch.log(torch.diag(L)))\n",
    "quadratic_part = -0.5 * features.double().matmul(torch.Tensor(y).double())\n",
    "const_part = -0.5 * len(L) * np.log(2 * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f6c709da",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpdf = const_part + log_determinant_part + quadratic_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ad3f6b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5043e-06, 3.9502e-01, 7.5263e+03,  ..., 5.0314e-01, 6.6063e+01,\n",
       "         1.5123e-06],\n",
       "        [1.2215e-01, 5.6694e-03, 5.6967e-02,  ..., 4.9925e-03, 4.4468e-03,\n",
       "         5.6896e-02],\n",
       "        [1.9087e+03, 4.6720e-02, 3.3142e-07,  ..., 3.9963e-02, 2.8434e-04,\n",
       "         3.3246e+03],\n",
       "        ...,\n",
       "        [1.5050e-01, 4.8296e-03, 4.7138e-02,  ..., 4.2068e-03, 3.4446e-03,\n",
       "         6.7190e-02],\n",
       "        [1.4760e+01, 3.2130e-03, 2.5051e-04,  ..., 2.5728e-03, 2.4421e-04,\n",
       "         7.5852e+00],\n",
       "        [1.3440e-06, 1.6353e-01, 1.1651e+04,  ..., 1.9963e-01, 3.0173e+01,\n",
       "         9.9014e-07]], dtype=torch.float64)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc8ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "af37caf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.zeros(2, 2, dtype=torch.float64).normal_()\n",
    "L = linalg.cholesky(torch.tril(S) @ torch.tril(S).t() + torch.eye(2))\n",
    "\n",
    "_batch_mahalanobis(L.unsqueeze(0), (features - means.unsqueeze(0)))/L.det() == _batch_mahalanobis(L, (features - means))/L.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602852f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89c2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d88503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nmmm] *",
   "language": "python",
   "name": "conda-env-nmmm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
